{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpu_available = tf.config.list_physical_devices('GPU')\n",
    "if gpu_available:\n",
    "    print(\"GPU is available.\")\n",
    "    print(\"Details:\", gpu_available)\n",
    "else:\n",
    "    print(\"GPU is not available.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-to-SQL Generation: Bridging Natural Language and Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **Introduction and Problem Statement**\n",
    "\n",
    "In recent years, data-driven decision-making has become crucial across industries. Despite the growing reliance on data, effectively retrieving information from databases remains challenging for non-technical stakeholders. Typically, databases rely on Structured Query Language (SQL) for data retrievalâ€”often a barrier for users unfamiliar with SQL syntax.\n",
    "\n",
    "To bridge this gap, the task of **\"Text-to-SQL generation\"** has emerged, focusing on automatically converting natural language (English) queries into accurate SQL statements. A successful Text-to-SQL system can significantly improve accessibility to complex data insights, empowering business analysts, product managers, and executives to directly query databases without relying on engineering resources.\n",
    "\n",
    "---\n",
    "\n",
    "## **Project Objective and Use Case**\n",
    "\n",
    "This project aims to build, evaluate, and deploy a high-quality Text-to-SQL model. Our specific use case targets business queries posed by non-technical users, converting them into executable SQL queries tailored for popular databases like **MySQL and SparkSQL**.\n",
    "\n",
    "**Example Scenario**:\n",
    "\n",
    "- **Natural Language Query:**  \n",
    "  `\"List all customers who spent more than $5000 in 2024*`\n",
    "\n",
    "- **Automatically Generated SQL Query:**\n",
    "```sql\n",
    "SELECT customer_name, SUM(amount_spent) as total_spent\n",
    "FROM transactions\n",
    "WHERE YEAR(transaction_date) = 2024\n",
    "GROUP BY customer_name\n",
    "HAVING total_spent > 5000;\n",
    "```\n",
    "\n",
    "##  Dataset and Evaluation Metrics\n",
    "\n",
    "To objectively measure performance and ensure rigorous benchmarking, this project leverages the [**Spider dataset**](https://yale-lily.github.io/spider/) ( [Yu et al., 2018](https://arxiv.org/abs/1809.08887)) , widely regarded as the gold standard for Text-to-SQL research. This dataset provides:\n",
    "\n",
    "- Diverse and complex natural language queries.\n",
    "- Corresponding SQL statements spanning multiple database schemas.\n",
    "- A rigorous framework to evaluate model accuracy, including **Exact Match (EM)** and **execution accuracy** metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## Project Approach and Methodology\n",
    "\n",
    "To achieve state-of-the-art performance, this project adopts a structured, incremental, and research-driven approach:\n",
    "\n",
    "### **Stage 1: Baseline Model Selection**\n",
    "- Evaluate top-performing open-source pretrained models such as **SQLCoder** and **CodeLlama**.\n",
    "- Establish initial performance benchmarks using standard metrics.\n",
    "\n",
    "### **Stage 2: Enhancing Performance through Prompt Engineering**\n",
    "- Systematically experiment with prompt variations.\n",
    "- Analyze how nuanced instructions impact SQL generation accuracy.\n",
    "\n",
    "### **Stage 3: Advanced Fine-tuning**\n",
    "- Employ Parameter-Efficient Fine-Tuning (PEFT) strategies:\n",
    "  - [**LoRA (Low-Rank Adaptation)**](https://arxiv.org/abs/2106.09685)\n",
    "  - [**qLoRA (Quantized LoRA)**](https://arxiv.org/abs/2305.14314)\n",
    "- Evaluate incremental performance improvements quantitatively.\n",
    "\n",
    "### **Stage 4: Performance Analysis and Comparison**\n",
    "- Perform comprehensive evaluation and rigorous statistical analysis.\n",
    "- Visualize comparative results clearly.\n",
    "\n",
    "### **Stage 5: Deployment and User Interaction**\n",
    "- Develop a user-friendly web application deployed via [**Hugging Face Spaces**](https://huggingface.co/spaces) with a **Gradio UI**, making the Text-to-SQL solution accessible to all stakeholders.\n",
    "\n",
    "---\n",
    "\n",
    "##  Why this Approach?\n",
    "\n",
    "This structured method ensures the following:\n",
    "\n",
    "- **Scientific rigor**: Clear benchmarking at each stage provides insights into what contributes most significantly to performance improvements.\n",
    "- **Practical applicability**: Continuous validation and deployment considerations ensure real-world effectiveness.\n",
    "- **Reproducibility and transparency**: Detailed documentation and open-source tools foster easy replication and extension by future researchers and practitioners.\n",
    "\n",
    "---\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "By the end of this comprehensive project, we aim to deliver:\n",
    "\n",
    "- A well-documented, reproducible pipeline for developing and deploying Text-to-SQL models.\n",
    "- A clear understanding of state-of-the-art fine-tuning and prompt-engineering techniques applicable to large language models (LLMs).\n",
    "- A demonstrably effective deployed application suitable for inclusion in a high-quality machine learning and NLP portfolio.\n",
    "\n",
    "---\n",
    "\n",
    "## References & Resources\n",
    "\n",
    "- Yu et al. (2018). [Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task](https://arxiv.org/abs/1809.08887)\n",
    "- Hu et al. (2021). [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n",
    "- Dettmers et al. (2023). [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **High-Level Project Plan (Step-by-Step)**\n",
    "\n",
    "The following structured approach ensures comprehensive exploration of the Text-to-SQL problem, systematically addressing all critical steps, from initial exploration to final deployment.\n",
    "\n",
    "---\n",
    "\n",
    "##  **Stage 1: Problem Framing & Environment Setup**\n",
    "\n",
    "- Clearly define the business use case.\n",
    "- Understand project objectives and goals.\n",
    "- Initial research on Text-to-SQL benchmarks (Spider dataset).\n",
    "\n",
    "**Tasks**:\n",
    "1. Clearly articulate use-case and objectives.\n",
    "2. Initial setup of Google Colab environment.\n",
    "3. Setup Hugging Face API and install necessary libraries.\n",
    "\n",
    "---\n",
    "\n",
    "## **Stage 2: Dataset Exploration**\n",
    "\n",
    "- Thorough exploration of the Spider dataset.\n",
    "- Statistical analysis and visualization to understand dataset properties and complexity.\n",
    "\n",
    "**Tasks**:\n",
    "1. Load and inspect the Spider dataset.\n",
    "2. Visualize and understand dataset structure, question types, SQL complexity.\n",
    "3. Identify dataset challenges (e.g., complex queries, cross-domain variations).\n",
    "\n",
    "---\n",
    "\n",
    "## **Stage 3: Baseline Implementation**\n",
    "\n",
    "- Evaluate performance using top open-source pretrained models:\n",
    "    - **SQLCoder**\n",
    "    - **CodeLlama**\n",
    "\n",
    "**Tasks**:\n",
    "1. Load pretrained models.\n",
    "2. Implement baseline predictions.\n",
    "3. Evaluate baseline performance (metrics: Exact Match, BLEU, SQL execution correctness).\n",
    "\n",
    "---\n",
    "\n",
    "## **Stage 4: Systematic Prompt Engineering**\n",
    "\n",
    "- Investigate the impact of prompt variations and strategies to improve accuracy.\n",
    "\n",
    "**Tasks**:\n",
    "1. Design multiple prompt templates and variations.\n",
    "2. Systematically test prompts and collect performance metrics.\n",
    "3. Identify best-performing prompts and analyze reasons for performance improvements.\n",
    "\n",
    "---\n",
    "\n",
    "## **Stage 5: Advanced Fine-tuning with PEFT**\n",
    "\n",
    "- Apply fine-tuning to further boost performance using parameter-efficient techniques:\n",
    "\n",
    "    - **LoRA (Low-Rank Adaptation)**\n",
    "    - **qLoRA (Quantized LoRA)**\n",
    "    - Additional PEFT methods (Prompt-Tuning, Prefix-Tuning, P-tuning v2)\n",
    "\n",
    "**Tasks**:\n",
    "1. Implement LoRA fine-tuning and evaluate improvements.\n",
    "2. Implement qLoRA fine-tuning and compare results.\n",
    "3. Experiment with Prompt-tuning and Prefix-tuning, comparing across methods.\n",
    "\n",
    "---\n",
    "\n",
    "##  **Stage 6: Comprehensive Performance Analysis**\n",
    "\n",
    "- Rigorous comparison and analysis of results across baseline, prompt engineering, and fine-tuning techniques.\n",
    "\n",
    "**Tasks**:\n",
    "1. Quantitatively compare Exact Match, BLEU, and SQL correctness.\n",
    "2. Create detailed visualizations (bar charts, line graphs, confusion matrices).\n",
    "3. Analyze trade-offs (accuracy vs. compute resources, inference time, scalability).\n",
    "\n",
    "---\n",
    "\n",
    "## **Stage 7: Application Deployment**\n",
    "\n",
    "- Deploy the final optimized Text-to-SQL model as a practical, user-friendly application.\n",
    "\n",
    "**Tasks**:\n",
    "1. Develop a web application interface using Gradio.\n",
    "2. Deploy the application on Hugging Face Spaces.\n",
    "3. Demonstrate real-world use-cases and model effectiveness.\n",
    "\n",
    "---\n",
    "\n",
    "## **Stage(-1): Additional Extensions & Recommended Enhancements** *(optional)*\n",
    "\n",
    "For deeper and broader research coverage, consider these optional enhancements:\n",
    "\n",
    "- **SQL Validation Integration**: Automate query correctness validation via MySQL/SparkSQL execution.\n",
    "- **Explainability Analysis**: Utilize attention visualization tools to interpret model predictions.\n",
    "- **Cross-Domain Generalization**: Evaluate model robustness on out-of-domain datasets.\n",
    "- **LLM Augmentation Techniques**: Investigate advanced approaches such as Retrieval-Augmented Generation (RAG) or Chain-of-Thought (CoT) prompting to enhance SQL generation.\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Stage 1: Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install essential libraries for the project\n",
    "!pip install transformers datasets accelerate peft gradio pandas matplotlib seaborn datasets huggingface_hub fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Hugging Face API and\n",
    "\n",
    "## After running this, youâ€™ll see a prompt to paste your Hugging Face API token. Enter it to authenticate your session.\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Stage 2: Dataset Exploration**\n",
    "\n",
    "**AIM** : understanding and visualizing the [Spider dataset](https://huggingface.co/datasets/xlangai/spider), which we'll use to benchmark and train our Text-to-SQL model.\n",
    "\n",
    "\n",
    "## **Paper Read**\n",
    "**[Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task](https://arxiv.org/abs/1809.08887)**\n",
    "\n",
    "\n",
    "**Dataset Summary** (from [dataset card](https://huggingface.co/datasets/xlangai/spider))\n",
    "\n",
    "Spider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students. The goal of the Spider challenge is to develop natural language interfaces to cross-domain databases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.1. Load the Spider Dataset from Huggingface\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# load spider dataset\n",
    "ds = load_dataset(\"xlangai/spider\")\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2. Dataset explorations\n",
    "df = pd.DataFrame(ds['train'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(df)\n",
    "print(f\"Number of samples in the dataset: {num_samples}\")\n",
    "\n",
    "num_unique_queries = df['query'].nunique()\n",
    "print(f\"Number of unique queries in the dataset: {num_unique_queries}\")\n",
    "\n",
    "db_distribution = df['db_id'].value_counts().head(10)\n",
    "print(\"\\nDatabase Distribution(top 10 queries):\")\n",
    "print(db_distribution)\n",
    "\n",
    "# Plot the top 10 most frequently used databases in the Spider dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=db_distribution.values, y=db_distribution.index, palette='Blues_d')\n",
    "plt.xlabel(\"Number of Queries\")\n",
    "plt.ylabel(\"Database ID\")\n",
    "plt.title(\"Top 10 Databases in Spider Dataset (by number of queries)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Stage 3: Initial Baseline Observations**\n",
    "\n",
    "## AIM:\n",
    "* Select and set up baseline models ([SQLCoder](https://github.com/defog-ai/sqlcoder?tab=readme-ov-file) and [CodeLlama](https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf)).\n",
    "* Run basic inference to generate SQL queries from natural language.\n",
    "* Evaluate initial performance qualitatively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and Set Up Models (SQLCoder and CodeLlama)\n",
    "print('started.....')\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "from accelerate import infer_auto_device_map, init_empty_weights\n",
    "\n",
    "# Select the model (\"SQLCoder\")\n",
    "model_name_sqlcoder = \"defog/sqlcoder-7b-2\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer_sqlcoder = AutoTokenizer.from_pretrained(model_name_sqlcoder)\n",
    "model_sqlcoder = AutoModelForCausalLM.from_pretrained(model_name_sqlcoder, device_map=\"auto\")\n",
    "\n",
    "# Setup pipeline for easy inference\n",
    "sqlcoder_pipeline = pipeline(\"text-generation\", model=model_sqlcoder, tokenizer=tokenizer_sqlcoder)\n",
    "\n",
    "# this takes forever on free google colab - T4 GPU with 15G GPU RAM\n",
    "# model_sqlcoder = AutoModelForCausalLM.from_pretrained(model_name_sqlcoder, device_map=\"auto\")\n",
    "\n",
    "# FIX#1: Load model with quantization and CPU offloading enabled ================\n",
    "\n",
    "'''\n",
    "# Set quantization configuration explicitly for NF4 quantization\n",
    "# Quantization configuration explicitly for NF4\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer_sqlcoder = AutoTokenizer.from_pretrained(model_name_sqlcoder)\n",
    "\n",
    "# First load the model structure (without actual weights) to infer the device map\n",
    "with init_empty_weights():\n",
    "    empty_model = AutoModelForCausalLM.from_pretrained(model_name_sqlcoder)\n",
    "\n",
    "# Infer device map based on empty model (correct approach)\n",
    "device_map = infer_auto_device_map(\n",
    "    empty_model,\n",
    "    max_memory={\"cpu\": \"30GB\"},\n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load the actual model using the explicitly generated device map\n",
    "model_sqlcoder = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_sqlcoder,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device_map,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# Set up the inference pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "sqlcoder_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_sqlcoder,\n",
    "    tokenizer=tokenizer_sqlcoder\n",
    ")\n",
    "'''\n",
    "\n",
    "'''\n",
    "# Fix#2: use smaller model\n",
    "\n",
    "model_name_small = \"Salesforce/codegen-2B-mono\"\n",
    "\n",
    "tokenizer_small = AutoTokenizer.from_pretrained(model_name_small)\n",
    "model_small = AutoModelForCausalLM.from_pretrained(model_name_small, device_map=\"auto\")\n",
    "\n",
    "sqlcoder_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_small,\n",
    "    tokenizer=tokenizer_small\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model (\"CodeLlama\")\n",
    "model_name_codellama = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer_codellama = AutoTokenizer.from_pretrained(model_name_codellama)\n",
    "model_codellama = AutoModelForCausalLM.from_pretrained(model_name_codellama, device_map=\"auto\")\n",
    "\n",
    "# Setup pipeline\n",
    "codellama_pipeline = pipeline(\"text-generation\", model=model_codellama, tokenizer=tokenizer_codellama)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic inference\n",
    "\n",
    "# Create test prompts and generate SQL queries:\n",
    "# Example business-related natural language prompt\n",
    "test_prompt = \"Generate an SQL query: How many customers purchased items over 100 dollars in February 2024?\"\n",
    "\n",
    "# SQLCoder inference\n",
    "response_sqlcoder = sqlcoder_pipeline(test_prompt, max_length=250, num_return_sequences=1)\n",
    "print(\"SQLCoder-generated SQL query:\\n\", response_sqlcoder[0]['generated_text'])\n",
    "\n",
    "# CodeLlama inference\n",
    "response_codellama = codellama_pipeline(test_prompt, max_length=250, num_return_sequences=1)\n",
    "print(\"\\nCodeLlama-generated SQL query:\\n\", response_codellama[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Evaluation\n",
    "\n",
    "measure three primary metrics for evaluating the performance of your models clearly and systematically:\n",
    "\n",
    "1. Exact Match (EM)\n",
    "Measures if the generated SQL exactly matches the reference SQL.\n",
    "\n",
    "2. BLEU Score\n",
    "Measures textual similarity between generated and reference queries, useful to identify near-correct queries.\n",
    "\n",
    "3. Execution-based Accuracy \n",
    "Evaluates if both the generated and the reference SQL produce identical results when executed against a real or simulated database. This metric directly measures practical usability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare Test Examples\n",
    "\n",
    "test_examples = pd.DataFrame({\n",
    "    \"question\": [\n",
    "        \"How many customers purchased items over 100 dollars in February 2024?\",\n",
    "        \"List all employees hired after 2022.\",\n",
    "        \"Show total revenue from sales in New York in March 2024.\"\n",
    "    ],\n",
    "    \"reference_sql\": [\n",
    "        \"SELECT COUNT(DISTINCT customer_id) FROM purchases WHERE price > 100 AND MONTH(purchase_date)=2 AND YEAR(purchase_date)=2024;\",\n",
    "        \"SELECT * FROM employees WHERE hire_date > '2022-12-31';\",\n",
    "        \"SELECT SUM(revenue) FROM sales WHERE city='New York' AND MONTH(sale_date)=3 AND YEAR(sale_date)=2024;\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Generate Predictions Automatically\n",
    "\n",
    "def generate_sql(pipe, question):\n",
    "    prompt = f\"Generate an SQL query: {question}\"\n",
    "    output = pipe(prompt, max_length=250, num_return_sequences=1)[0]['generated_text']\n",
    "    return output.strip()\n",
    "\n",
    "# Generate predictions\n",
    "test_examples[\"sqlcoder_prediction\"] = test_examples[\"question\"].apply(lambda q: generate_sql(sqlcoder_pipeline, q))\n",
    "test_examples[\"codellama_prediction\"] = test_examples[\"question\"].apply(lambda q: generate_sql(codellama_pipeline, q))\n",
    "\n",
    "# Check predictions\n",
    "test_examples.head()\n",
    "\n",
    "\n",
    "# Step 3: Compute Quantitative Metrics (Exact Match and BLEU)\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import numpy as np\n",
    "\n",
    "# Calculate Exact Match and BLEU\n",
    "def evaluate_metrics(reference, prediction):\n",
    "    em = int(reference.strip().lower() == prediction.strip().lower())\n",
    "    bleu = sentence_bleu([reference.split()], prediction.split())\n",
    "    return em, bleu\n",
    "\n",
    "# Evaluate predictions\n",
    "for model in [\"sqlcoder_prediction\", \"codellama_prediction\"]:\n",
    "    em_scores = []\n",
    "    bleu_scores = []\n",
    "    for idx, row in test_examples.iterrows():\n",
    "        em, bleu = evaluate_metrics(row[\"reference_sql\"], row[model])\n",
    "        em_scores.append(em)\n",
    "        bleu_scores.append(bleu)\n",
    "    test_examples[f\"{model}_EM\"] = em_scores\n",
    "    test_examples[f\"{model}_BLEU\"] = bleu_scores\n",
    "\n",
    "# Display evaluation results\n",
    "print(test_examples[[\"question\", \"sqlcoder_prediction_EM\", \"sqlcoder_prediction_BLEU\", \n",
    "                     \"codellama_prediction_EM\", \"codellama_prediction_BLEU\"]])\n",
    "\n",
    "\n",
    "# Step 4: Summarize Results Clearly\n",
    "'''\n",
    "| Metric                     | SQLCoder | CodeLlama |\n",
    "|----------------------------|----------|-----------|\n",
    "| Exact Match (EM) (%)       | x %      | y %       |\n",
    "| Average BLEU Score         | x.xx     | y.yy      |\n",
    "| Execution-based Accuracy (%) | x %      | y %       |\n",
    "\n",
    "**Observations**:  \n",
    "- Model \"X\" performed better in Exact Match, suggesting high precision in generating SQL queries.\n",
    "- Model \"Y\" had better BLEU scores, indicating closer text similarity to reference queries.\n",
    "- Execution accuracy reflects practical usability and correctness.\n",
    "\n",
    "**Next Steps**:  \n",
    "These baseline evaluations set clear benchmarks for improvements through prompt engineering and fine-tuning.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Stage 4: Prompt Engineering**\n",
    "\n",
    "testing different clearly defined prompt structures, such as:\n",
    "\n",
    "1. Basic prompt\n",
    "\n",
    "2. Instruction-based prompt\n",
    "\n",
    "3. Few-shot prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prompt Templates\n",
    "\n",
    "prompts = {\n",
    "    \"basic_prompt\": \"Write SQL query for: {question}\",\n",
    "    \n",
    "    \"instruction_prompt\": \"Given the following question, produce the corresponding SQL query:\\nQuestion: {question}\\nSQL query:\",\n",
    "    \n",
    "    \"few_shot_prompt\": (\n",
    "        \"Question: How many employees joined after January 2020?\\n\"\n",
    "        \"SQL: SELECT COUNT(*) FROM employees WHERE join_date > '2020-01-31';\\n\\n\"\n",
    "        \"Question: List top 3 customers by revenue.\\n\"\n",
    "        \"SQL: SELECT customer_name, SUM(revenue) FROM sales GROUP BY customer_name ORDER BY SUM(revenue) DESC LIMIT 3;\\n\\n\"\n",
    "        \"Question: {question}\\nSQL:\"\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "# Step 2: Run prompt experiments systematically\n",
    "test_questions = [\n",
    "    \"How many customers spent over 1000 dollars in March 2024?\",\n",
    "    \"List the names of employees hired before 2021.\",\n",
    "    \"Show total sales by region for Q1 2024.\"\n",
    "]\n",
    "\n",
    "# Dictionary to store generated SQL for analysis\n",
    "results = {}\n",
    "\n",
    "# Evaluate all prompts systematically\n",
    "for prompt_name, prompt_template in prompts.items():\n",
    "    print(f\"\\nEvaluating prompt type: {prompt_name}\\n{'-'*40}\")\n",
    "    results[prompt_name] = []\n",
    "    for question in test_questions:\n",
    "        full_prompt = prompt_template.format(question=question)\n",
    "        generated = codegen_pipeline(full_prompt, max_length=150, do_sample=False)\n",
    "        sql_output = generated[0]['generated_text'][len(full_prompt):].strip()\n",
    "        print(f\"Question: {question}\\nGenerated SQL: {sql_output}\\n\")\n",
    "        results[prompt_name].append({\"question\": question, \"generated_sql\": sql_output})\n",
    "\n",
    "\n",
    "# Step 3: Qualitative Analysis\n",
    "\n",
    "'''\n",
    "## Prompt Engineering Results (Qualitative Analysis)\n",
    "\n",
    "### Basic Prompt:\n",
    "- **Strengths**: (e.g., simple, direct)\n",
    "- **Weaknesses**: (e.g., less accurate, incomplete queries)\n",
    "\n",
    "### Instruction Prompt:\n",
    "- **Strengths**: (e.g., improved accuracy)\n",
    "- **Weaknesses**: (e.g., longer outputs, slower inference)\n",
    "\n",
    "### Few-shot Prompt:\n",
    "- **Strengths**: (e.g., best accuracy, higher consistency)\n",
    "- **Weaknesses**: (e.g., requires more tokens)\n",
    "\n",
    "### Best Performing Prompt:\n",
    "- Based on qualitative review, the best-performing prompt structure is: [specify your conclusion clearly here].\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Stage 5: Fine-Tuning (Advanced Model Optimization)**\n",
    "\n",
    "This step focus on parameter-efficient fine-tuning methods (PEFT), which optimize large models without requiring vast computational resources. The key PEFT methods you'll explore include:\n",
    "\n",
    "1. **Low-Rank Adaptation (LoRA)**\n",
    "\n",
    "**Description:**\n",
    "LoRA introduces trainable low-rank matrices into the existing model layers, significantly reducing memory usage and speeding up fine-tuning.\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "* Minimal GPU/CPU memory usage.\n",
    "\n",
    "Rapid training and inference.\n",
    "\n",
    "* Easy integration with existing transformer models.\n",
    "\n",
    "**Key Resource (Paper):**\n",
    "\n",
    "[LoRA: Low-Rank Adaptation of Large Language Models (Hu et al., 2021)](https://arxiv.org/abs/2106.09685)\n",
    "\n",
    "2. **Quantized LoRA (qLoRA)**\n",
    "**Description:**\n",
    "qLoRA combines quantization (4-bit or 8-bit) with LoRA, allowing fine-tuning of large models on hardware with limited memory (e.g., Google Colab).\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "* Ultra-low memory footprint.\n",
    "\n",
    "* Enables fine-tuning of very large models on inexpensive hardware.\n",
    "\n",
    "* **Key Resource (Paper):**\n",
    "\n",
    "[QLoRA: Efficient Finetuning of Quantized LLMs (Dettmers et al., 2023)](https://arxiv.org/abs/2305.14314)\n",
    "\n",
    "3. **Prompt-Tuning and P-Tuning v2 (Alternative PEFT methods)**\n",
    "**Description:**\n",
    "Prompt-tuning methods train only a small number of parameters associated with the input prompt, rather than the entire model.\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "* Very efficient training and deployment.\n",
    "\n",
    "* Effective when you have limited computational resources.\n",
    "\n",
    "**Key Resource (Paper):**\n",
    "\n",
    "[P-tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/abs/2110.07602)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1:  Environment Setup\n",
    "!pip install transformers peft accelerate datasets bitsandbytes\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Load Dataset (Spider Dataset)\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"spider\")\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']\n",
    "\n",
    "# Step 3: Tokenize Data\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"Salesforce/codegen-2B-mono\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    inputs = tokenizer(example[\"question\"], truncation=True, max_length=256, padding=\"max_length\")\n",
    "    labels = tokenizer(example[\"query\"], truncation=True, max_length=256, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "# Step 4: Configure LoRA/qLoRA\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                   # Low-rank dimension\n",
    "    lora_alpha=32,          # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], # Typically target attention layers\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Load quantized model with qLoRA for efficiency\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "\n",
    "# Step 5: Fine-Tune the Model\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"finetuned_model\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    logging_steps=100,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Stage 6: deployment on gradio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def generate_sql(question):\n",
    "    prompt = f\"Generate SQL query: {question}\"\n",
    "    response = pipe(prompt, max_length=200)[0]['generated_text']\n",
    "    return response\n",
    "\n",
    "iface = gr.Interface(fn=generate_sql,\n",
    "                     inputs=\"text\",\n",
    "                     outputs=\"text\",\n",
    "                     title=\"Text to SQL Generator\",\n",
    "                     description=\"Convert natural language questions to SQL queries.\")\n",
    "\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
